{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a07f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32383982",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7269f5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if driver.session_id is None:\n",
    "  print(\"The Webdriver session is not active.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8973c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.goodreads.com/list/best_of_year/2023?id=183940.Best_Books_of_2023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2346ecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "def html_save(url):\n",
    "    \"\"\"\n",
    "    Saves the HTML content of the given URL to a file and returns the file path.\n",
    "    \"\"\"\n",
    "    # Initialize the browser\n",
    "    browser = webdriver.Firefox()  # Ensure you have the Firefox WebDriver installed\n",
    "    browser.get(url)\n",
    "    \n",
    "    # Get the HTML content from the webpage\n",
    "    html_content = browser.page_source\n",
    "    browser.close()\n",
    "\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Use the current working directory to save the file\n",
    "    html_save_path = Path(os.getcwd()) / \"content.txt\"\n",
    "\n",
    "    # Write the prettified HTML content to the file\n",
    "    with open(html_save_path, 'wt', encoding='utf-8') as html_file:\n",
    "        html_file.write(soup.prettify())\n",
    "\n",
    "    print(f\"HTML content saved to: {html_save_path}\")\n",
    "    return html_save_path  # Return the path to the saved file\n",
    "\n",
    "\n",
    "def getLanguage(file_path):\n",
    "    \"\"\"\n",
    "    Extracts the language from the JSON data within the HTML content of the file.\n",
    "    Deletes the file after extraction.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the HTML content from the file\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            html_content = file.read()\n",
    "\n",
    "        # Locate the <script> tag with type=\"application/ld+json\" that contains the language information\n",
    "        start = html_content.find('<script type=\"application/ld+json\">') + len('<script type=\"application/ld+json\">')\n",
    "        end = html_content.find('</script>', start)\n",
    "        json_data = html_content[start:end].strip()\n",
    "\n",
    "        # Parse the JSON data\n",
    "        parsed_data = json.loads(json_data)\n",
    "\n",
    "        # Extract the \"inLanguage\" field\n",
    "        language = parsed_data.get(\"inLanguage\", \"N/A\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting language: {e}\")\n",
    "        language = \"N/A\"\n",
    "\n",
    "    # Delete the file\n",
    "    os.remove(file_path)\n",
    "    return language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afccfcd4",
   "metadata": {},
   "source": [
    "## Srape for Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1898177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "def get_ranks_for_page(driver, page_url):\n",
    "    \"\"\"\n",
    "    Scrape the rankings from a specific Goodreads page.\n",
    "    \n",
    "    Args:\n",
    "    - driver: WebDriver instance\n",
    "    - page_url: URL of the Goodreads page to scrape\n",
    "    \n",
    "    Returns:\n",
    "    - ranks: List of ranks extracted from the page\n",
    "    \"\"\"\n",
    "    ranks = []\n",
    "    try:\n",
    "        print(f'Processing page: {page_url}')\n",
    "        driver.get(page_url)\n",
    "        time.sleep(5)  # Allow the page to load\n",
    "\n",
    "        rows = driver.find_elements(By.XPATH, '//*[@id=\"all_votes\"]/table/tbody/tr')\n",
    "        print(f'Number of rows found: {len(rows)}')\n",
    "\n",
    "        for row_index, row in enumerate(rows, start=1):\n",
    "            try:\n",
    "                rank = row.find_element(By.XPATH, './td[@class=\"number\"]').text.strip()\n",
    "                ranks.append(rank)\n",
    "\n",
    "                # Print details\n",
    "                print(f\"Row {row_index}: Rank: {rank}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting rank on row {row_index}: {e}\")\n",
    "                ranks.append(\"N/A\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing page {page_url}: {e}\")\n",
    "    \n",
    "    return ranks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ec9486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scraping(baseURL, log_name, num_web_page):\n",
    "\n",
    "    # Create and open the log file for appending\n",
    "    log_file = open(log_name, \"a\", encoding=\"utf-8\")\n",
    "\n",
    "    # Redirect print and errors to the log file\n",
    "    sys.stdout = log_file\n",
    "    sys.stderr = log_file\n",
    "\n",
    "    # Initialize WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Initialize lists for storing data\n",
    "    titles, authors, genres = [], [], []\n",
    "    languages, average_ratings, num_ratings = [], [], []\n",
    "    publication_dates, pages, currently_reading, want_to_read = [], [], [], []\n",
    "    all_ranks, book_links = [], []\n",
    "\n",
    "    # Loop through pages\n",
    "    for i in range(1, num_web_page + 1):  \n",
    "        print(f'Processing page {i}')\n",
    "        sys.stdout.flush()\n",
    "        targetURL = f\"{baseURL}&page={i}\"\n",
    "        driver.get(targetURL)\n",
    "        time.sleep(5)  # Allow the page to load\n",
    "        \n",
    "        ranks = get_ranks_for_page(driver, targetURL)\n",
    "        reattempt = 0\n",
    "        while (ranks[0] == '' and reattempt < 3):\n",
    "            print(f\"Reattempting ranks collection for page {i}, attempt {reattempt + 1}\")\n",
    "            sys.stdout.flush()\n",
    "            ranks = get_ranks_for_page(driver, targetURL)\n",
    "            reattempt += 1\n",
    "\n",
    "        if ranks[0] == '':\n",
    "            print(f\"Failed to collect ranks for page {i} after {reattempt} attempts\")\n",
    "            sys.stdout.flush()\n",
    "            continue\n",
    "\n",
    "        print(f'For page {i}, collected ranks: {ranks}')\n",
    "        sys.stdout.flush()\n",
    "        all_ranks.extend(ranks)\n",
    "\n",
    "        rows = driver.find_elements(By.XPATH, '//*[@id=\"all_votes\"]/table/tbody/tr')\n",
    "\n",
    "        # Extract book links from each row\n",
    "        for row in rows:\n",
    "            try:\n",
    "                href = row.find_element(By.XPATH, './td[3]/a').get_attribute('href')\n",
    "                book_links.append(href)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting rank or book link: {e}\")\n",
    "                sys.stdout.flush()\n",
    "                book_links.append(\"N/A\")\n",
    "\n",
    "    # Visit each book link to collect details\n",
    "    for rank, href in zip(all_ranks, book_links):\n",
    "        try:\n",
    "            driver.get(href)\n",
    "            time.sleep(2)  # Allow the page to load\n",
    "\n",
    "            # Extract book details\n",
    "            title = driver.find_element(By.XPATH, '//h1[@class=\"Text Text__title1\"]').text\n",
    "            author = driver.find_element(By.XPATH, '//span[@class=\"ContributorLink__name\"]').text\n",
    "            genre = driver.find_element(By.XPATH, '(//div[@class=\"BookPageMetadataSection__genres\"]//span[@class=\"Button__labelItem\"])[1]').text\n",
    "            avg_rating = driver.find_element(By.XPATH, '//div[@class=\"RatingStatistics__rating\"]').text\n",
    "            num_rating = driver.find_element(By.XPATH, '//div[@class=\"RatingStatistics__meta\"]').text.split(\" ratings\")[0].strip()\n",
    "            page = driver.find_element(By.XPATH, '//p[@data-testid=\"pagesFormat\"]').text.split(\" pages\")[0]\n",
    "            pub_date = driver.find_element(By.XPATH, '//p[@data-testid=\"publicationInfo\"]').text.replace(\"First published\", \"\").strip()\n",
    "            reading_now = driver.find_element(By.XPATH, '//div[@data-testid=\"currentlyReadingSignal\"]').text.split(\" people\")[0]\n",
    "            want_read = driver.find_element(By.XPATH, '//div[@data-testid=\"toReadSignal\"]').text.split(\" people\")[0]\n",
    "\n",
    "            # Use html_save and getLanguage to extract language\n",
    "            file_path = html_save(href)\n",
    "            language = getLanguage(file_path)\n",
    "\n",
    "            # Append details to lists\n",
    "            titles.append(title)\n",
    "            authors.append(author)\n",
    "            genres.append(genre)\n",
    "            average_ratings.append(avg_rating)\n",
    "            num_ratings.append(num_rating)\n",
    "            pages.append(page)\n",
    "            publication_dates.append(pub_date)\n",
    "            currently_reading.append(reading_now)\n",
    "            want_to_read.append(want_read)\n",
    "            languages.append(language)\n",
    "\n",
    "            # Log book details immediately\n",
    "            print(f\"Processed Book:\")\n",
    "            print(f\"  Rank: {rank}\")\n",
    "            print(f\"  Book Link: {href}\")\n",
    "            print(f\"  Title: {title}\")\n",
    "            print(f\"  Author: {author}\")\n",
    "            print(f\"  Genre: {genre}\")\n",
    "            print(f\"  Average Rating: {avg_rating}\")\n",
    "            print(f\"  Number of Ratings: {num_rating}\")\n",
    "            print(f\"  Number of Pages: {page}\")\n",
    "            print(f\"  Publication Date: {pub_date}\")\n",
    "            print(f\"  Currently Reading: {reading_now}\")\n",
    "            print(f\"  Want to Read: {want_read}\")\n",
    "            print(f\"  Language: {language}\")\n",
    "            print(\"-\" * 40)\n",
    "            sys.stdout.flush()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing book at {href}: {e}\")\n",
    "            sys.stdout.flush()\n",
    "            titles.append(\"N/A\")\n",
    "            authors.append(\"N/A\")\n",
    "            genres.append(\"N/A\")\n",
    "            average_ratings.append(\"N/A\")\n",
    "            num_ratings.append(\"N/A\")\n",
    "            pages.append(\"N/A\")\n",
    "            publication_dates.append(\"N/A\")\n",
    "            currently_reading.append(\"N/A\")\n",
    "            want_to_read.append(\"N/A\")\n",
    "            languages.append(\"N/A\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Restore stdout and stderr\n",
    "    sys.stdout = sys.__stdout__\n",
    "    sys.stderr = sys.__stderr__\n",
    "    log_file.close()\n",
    "\n",
    "    # Create the DataFrame\n",
    "    return pd.DataFrame({\n",
    "        \"Rank\": all_ranks,\n",
    "        \"Book Link\": book_links,\n",
    "        \"Title\": titles,\n",
    "        \"Author\": authors,\n",
    "        \"Genre\": genres,\n",
    "        \"Language\": languages,\n",
    "        \"Average Rating\": average_ratings,\n",
    "        \"Number of Ratings\": num_ratings,\n",
    "        \"Publication Date\": publication_dates,\n",
    "        \"Number of Pages\": pages,\n",
    "        \"Currently Reading\": currently_reading,\n",
    "        \"Want to Read\": want_to_read,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde94ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_1 = data_scraping(\"https://www.goodreads.com/list/best_of_year/2023?id=183940.Best_Books_of_2023\", \"scraping_log.txt\", 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d88157",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_1.to_csv(\"data/books_data_1.csv\", index=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4f2111",
   "metadata": {},
   "source": [
    "## Scrape for Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06c9e732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "def get_book_links(baseURL, num_web_page, log_name):\n",
    "    \"\"\"\n",
    "    Function to extract book links from the given pages and log the process.\n",
    "    \"\"\"\n",
    "    # Open the log file for appending\n",
    "    log_file = open(log_name, \"a\", encoding=\"utf-8\")\n",
    "\n",
    "    # Redirect print and errors to the log file\n",
    "    sys.stdout = log_file\n",
    "    sys.stderr = log_file\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    book_links = []\n",
    "\n",
    "    # Loop through pages to get all book links\n",
    "    # Loop through pages\n",
    "    for i in range(1, num_web_page + 1):  \n",
    "        print(f'Processing page {i}')\n",
    "        sys.stdout.flush()\n",
    "        targetURL = f\"{baseURL}?page={i}\"\n",
    "        driver.get(targetURL)\n",
    "        time.sleep(5)  # Allow the page to load\n",
    "        rows = driver.find_elements(By.XPATH, '//table[@class=\"tableList\"]/tbody/tr')\n",
    "        print(f'Number of rows found: {len(rows)}')\n",
    "\n",
    "        # Extract book links from each row\n",
    "        for row in rows:\n",
    "            try:\n",
    "                href = row.find_element(By.XPATH, './td[2]/a').get_attribute('href')\n",
    "                book_links.append(href)\n",
    "                print(f'Current href: {href}')\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting rank or book link: {e}\")\n",
    "                sys.stdout.flush()\n",
    "                book_links.append(\"N/A\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Restore stdout and stderr\n",
    "    sys.stdout = sys.__stdout__\n",
    "    sys.stderr = sys.__stderr__\n",
    "    log_file.close()\n",
    "\n",
    "    return book_links\n",
    "\n",
    "\n",
    "def data_scraping_2(book_links, log_name, start, end):\n",
    "    \"\"\"\n",
    "    Function to scrape book details from each book link, including language using html_save and getLanguage.\n",
    "    Retries up to three times with a wait time before retrying if data collection fails before moving to the next book.\n",
    "    Sets 'N/A' for fields that cannot be found.\n",
    "    \"\"\"\n",
    "    # Create and open the log file for appending\n",
    "    log_file = open(log_name, \"a\", encoding=\"utf-8\")\n",
    "\n",
    "    # Redirect print and errors to the log file\n",
    "    sys.stdout = log_file\n",
    "    sys.stderr = log_file\n",
    "\n",
    "    # Initialize WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    # Initialize lists for storing data\n",
    "    titles, authors, genres = [], [], []\n",
    "    languages, average_ratings, num_ratings = [], [], []\n",
    "    publication_dates, pages, currently_reading, want_to_read = [], [], [], []\n",
    "\n",
    "    # Start indexing from the start index\n",
    "    i = start  \n",
    "    for href in book_links[start:end]:\n",
    "        retries = 0\n",
    "        success = False\n",
    "        while retries < 3 and not success:\n",
    "            try:\n",
    "                driver.get(href)\n",
    "                time.sleep(2)  # Allow initial page load\n",
    "\n",
    "                # Extract book details\n",
    "                try:\n",
    "                    title = wait.until(EC.presence_of_element_located((By.XPATH, '//h1[@class=\"Text Text__title1\"]'))).text\n",
    "                except Exception:\n",
    "                    title = \"N/A\"\n",
    "\n",
    "                try:\n",
    "                    author = wait.until(EC.presence_of_element_located((By.XPATH, '//span[@class=\"ContributorLink__name\"]'))).text\n",
    "                except Exception:\n",
    "                    author = \"N/A\"\n",
    "\n",
    "                try:\n",
    "                    genre = wait.until(EC.presence_of_element_located((By.XPATH, '(//div[@class=\"BookPageMetadataSection__genres\"]//span[@class=\"Button__labelItem\"])[1]'))).text\n",
    "                except Exception:\n",
    "                    genre = \"N/A\"\n",
    "\n",
    "                try:\n",
    "                    avg_rating = wait.until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"RatingStatistics__rating\"]'))).text\n",
    "                except Exception:\n",
    "                    avg_rating = \"N/A\"\n",
    "\n",
    "                try:\n",
    "                    num_rating = wait.until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"RatingStatistics__meta\"]'))).text.split(\" ratings\")[0].strip()\n",
    "                except Exception:\n",
    "                    num_rating = \"N/A\"\n",
    "\n",
    "                try:\n",
    "                    page = wait.until(EC.presence_of_element_located((By.XPATH, '//p[@data-testid=\"pagesFormat\"]'))).text.split(\" pages\")[0]\n",
    "                except Exception:\n",
    "                    page = \"N/A\"\n",
    "\n",
    "                try:\n",
    "                    pub_date = wait.until(EC.presence_of_element_located((By.XPATH, '//p[@data-testid=\"publicationInfo\"]'))).text.replace(\"First published\", \"\").strip()\n",
    "                except Exception:\n",
    "                    pub_date = \"N/A\"\n",
    "\n",
    "                try:\n",
    "                    reading_now = wait.until(EC.presence_of_element_located((By.XPATH, '//div[@data-testid=\"currentlyReadingSignal\"]'))).text.split(\" people\")[0]\n",
    "                except Exception:\n",
    "                    reading_now = \"N/A\"\n",
    "\n",
    "                try:\n",
    "                    want_read = wait.until(EC.presence_of_element_located((By.XPATH, '//div[@data-testid=\"toReadSignal\"]'))).text.split(\" people\")[0]\n",
    "                except Exception:\n",
    "                    want_read = \"N/A\"\n",
    "\n",
    "                try:\n",
    "                    # Use html_save and getLanguage to extract language\n",
    "                    file_path = html_save(href)\n",
    "                    language = getLanguage(file_path)\n",
    "                except Exception:\n",
    "                    language = \"N/A\"\n",
    "\n",
    "                # Append details to lists\n",
    "                titles.append(title)\n",
    "                authors.append(author)\n",
    "                genres.append(genre)\n",
    "                average_ratings.append(avg_rating)\n",
    "                num_ratings.append(num_rating)\n",
    "                pages.append(page)\n",
    "                publication_dates.append(pub_date)\n",
    "                currently_reading.append(reading_now)\n",
    "                want_to_read.append(want_read)\n",
    "                languages.append(language)\n",
    "\n",
    "                # Log book details immediately\n",
    "                print(f\"Processed Book {i + 1}:\")\n",
    "                print(f\"  Book Link: {href}\")\n",
    "                print(f\"  Title: {title}\")\n",
    "                print(f\"  Author: {author}\")\n",
    "                print(f\"  Genre: {genre}\")\n",
    "                print(f\"  Average Rating: {avg_rating}\")\n",
    "                print(f\"  Number of Ratings: {num_rating}\")\n",
    "                print(f\"  Number of Pages: {page}\")\n",
    "                print(f\"  Publication Date: {pub_date}\")\n",
    "                print(f\"  Currently Reading: {reading_now}\")\n",
    "                print(f\"  Want to Read: {want_read}\")\n",
    "                print(f\"  Language: {language}\")\n",
    "                print(\"-\" * 40)\n",
    "                sys.stdout.flush()\n",
    "                success = True  # Mark success if no exception occurred\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error processing book at {href}, attempt {retries}: {e}\")\n",
    "                sys.stdout.flush()\n",
    "                if retries < 3:\n",
    "                    wait_time = retries * 5  # Increasing wait time with each retry (e.g., 5s, 10s, 15s)\n",
    "                    print(f\"Waiting for {wait_time} seconds before retrying...\")\n",
    "                    time.sleep(wait_time)\n",
    "                if retries == 3:\n",
    "                    # If all retries fail, append \"N/A\" values for all fields\n",
    "                    print(f\"Failed to process book at {href} after 3 attempts. Moving on.\")\n",
    "                    titles.append(\"N/A\")\n",
    "                    authors.append(\"N/A\")\n",
    "                    genres.append(\"N/A\")\n",
    "                    average_ratings.append(\"N/A\")\n",
    "                    num_ratings.append(\"N/A\")\n",
    "                    pages.append(\"N/A\")\n",
    "                    publication_dates.append(\"N/A\")\n",
    "                    currently_reading.append(\"N/A\")\n",
    "                    want_to_read.append(\"N/A\")\n",
    "                    languages.append(\"N/A\")\n",
    "\n",
    "        # Increment counter after all retries, regardless of success\n",
    "        i += 1\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Restore stdout and stderr\n",
    "    sys.stdout = sys.__stdout__\n",
    "    sys.stderr = sys.__stderr__\n",
    "    log_file.close()\n",
    "\n",
    "    # Create the DataFrame\n",
    "    return pd.DataFrame({\n",
    "        \"Book Link\": book_links[start:end],\n",
    "        \"Title\": titles,\n",
    "        \"Author\": authors,\n",
    "        \"Genre\": genres,\n",
    "        \"Language\": languages,\n",
    "        \"Average Rating\": average_ratings,\n",
    "        \"Number of Ratings\": num_ratings,\n",
    "        \"Publication Date\": publication_dates,\n",
    "        \"Number of Pages\": pages,\n",
    "        \"Currently Reading\": currently_reading,\n",
    "        \"Want to Read\": want_to_read,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8054605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "baseURL = \"https://www.goodreads.com/author/list/3389.Stephen_King\"\n",
    "log_name = \"scraping_log_2.txt\"\n",
    "num_web_page = 88\n",
    "\n",
    "# Step 1: Get all book links\n",
    "book_links = get_book_links(baseURL, num_web_page, log_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14d079f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(book_links))\n",
    "# 2616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "651370ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2616\n",
      "Data saved to ../data/book_scrape_2/books_df2_p1.csv\n",
      "Data saved to ../data/book_scrape_2/books_df2_p2.csv\n",
      "Data saved to ../data/book_scrape_2/books_df2_p3.csv\n",
      "Data saved to ../data/book_scrape_2/books_df2_p5.csv\n",
      "Data saved to ../data/book_scrape_2/books_df2_p1.csv\n",
      "Data saved to ../data/book_scrape_2/books_df2_p2.csv\n",
      "Data saved to ../data/book_scrape_2/books_df2_p3.csv\n",
      "Data saved to ../data/book_scrape_2/books_df2_p5.csv\n",
      "Data saved to ./data/book_scrape_2/books_df2_p1.csv\n",
      "Data saved to ./data/book_scrape_2/books_df2_p2.csv\n",
      "Data saved to ./data/book_scrape_2/books_df2_p3.csv\n",
      "Data saved to ./data/book_scrape_2/books_df2_p5.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Scrape book details\n",
    "books_df_2_p1 = data_scraping_2(book_links, log_name, 0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab9eb6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p2 = data_scraping_2(book_links, log_name, 100, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93994862",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p3 = data_scraping_2(book_links, log_name, 200, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dc96aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p5 = data_scraping_2(book_links, log_name, 400, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157bc8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p6 = data_scraping_2(book_links, log_name, 500, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa19731",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p7 = data_scraping_2(book_links, log_name, 600, 700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f84640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p8 = data_scraping_2(book_links, log_name, 700, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95529114",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p9 = data_scraping_2(book_links, log_name, 800, 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bf069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p10 = data_scraping_2(book_links, log_name, 900, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p11 = data_scraping_2(book_links, log_name, 1000, 1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p12 = data_scraping_2(book_links, log_name, 1100, 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb764530",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p17 = data_scraping_2(book_links, log_name, 1600, 1700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79913e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p26 = data_scraping_2(book_links, log_name, 2500, 2600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dbf4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p27 = data_scraping_2(book_links, log_name, 2600, 2627)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8f5927",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p13 = data_scraping_2(book_links, log_name, 1200, 1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e542e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p4 = data_scraping_2(book_links, log_name, 300, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75ac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p21 = data_scraping_2(book_links, log_name, 2000, 2100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641396b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p22 = data_scraping_2(book_links, log_name, 2100, 2200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a7290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p23 = data_scraping_2(book_links, log_name, 2200, 2300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac75d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p24 = data_scraping_2(book_links, log_name, 2300, 2400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a5ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p25 = data_scraping_2(book_links, log_name, 2400, 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c881a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p18 = data_scraping_2(book_links, log_name, 1700, 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6485a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p19 = data_scraping_2(book_links, log_name, 1800, 1900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36b7e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p20 = data_scraping_2(book_links, log_name, 1900, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9292c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p14 = data_scraping_2(book_links, log_name, 1300, 1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b95ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p15 = data_scraping_2(book_links, log_name, 1400, 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa76c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_2_p16 = data_scraping_2(book_links, log_name, 1500, 1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62b33b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def save_to_csv(data, output_dir, file_name):\n",
    "    \"\"\"\n",
    "    Saves a DataFrame to a CSV file in the specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame or dict): The data to be saved. If it's a dictionary, it will be converted to a DataFrame.\n",
    "        output_dir (str): The directory where the CSV file should be saved.\n",
    "        file_name (str): The name of the CSV file (without extension).\n",
    "    \n",
    "    Returns:\n",
    "        str: The full path to the saved CSV file.\n",
    "    \"\"\"\n",
    "    # Ensure the data is a DataFrame\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        data = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create the full file path\n",
    "    file_path = os.path.join(output_dir, f\"{file_name}.csv\")\n",
    "    \n",
    "    # Save the DataFrame to CSV\n",
    "    data.to_csv(file_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Data saved to {file_path}\")\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb84d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/book_scrape_2/books_df2_p5.csv'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_to_csv(books_df_2_p1, './data/book_scrape_2', 'books_df2_p1')\n",
    "save_to_csv(books_df_2_p2, './data/book_scrape_2', 'books_df2_p2')\n",
    "save_to_csv(books_df_2_p3, './data/book_scrape_2', 'books_df2_p3')\n",
    "save_to_csv(books_df_2_p5, './data/book_scrape_2', 'books_df2_p5')\n",
    "save_to_csv(books_df_2_p6, '../data/book_scrape_2', 'books_df2_p6')\n",
    "save_to_csv(books_df_2_p7, '../data/book_scrape_2', 'books_df2_p7')\n",
    "save_to_csv(books_df_2_p8, '../data/book_scrape_2', 'books_df2_p8')\n",
    "save_to_csv(books_df_2_p9, '../data/book_scrape_2', 'books_df2_p9')\n",
    "save_to_csv(books_df_2_p10, '../data/book_scrape_2', 'books_df2_p10')\n",
    "save_to_csv(books_df_2_p11, '../data/book_scrape_2', 'books_df2_p11')\n",
    "save_to_csv(books_df_2_p12, '../data/book_scrape_2', 'books_df2_p12')\n",
    "save_to_csv(books_df_2_p17, '../data/book_scrape_2', 'books_df2_p17')\n",
    "save_to_csv(books_df_2_p26, '../data/book_scrape_2', 'books_df2_p26')\n",
    "save_to_csv(books_df_2_p27, '../data/book_scrape_2', 'books_df2_p27')\n",
    "save_to_csv(books_df_2_p13, '../data/book_scrape_2', 'books_df2_p13')\n",
    "save_to_csv(books_df_2_p4, '../data/book_scrape_2', 'books_df2_p4')\n",
    "save_to_csv(books_df_2_p21, '../data/book_scrape_2', 'books_df2_p21')\n",
    "save_to_csv(books_df_2_p22, '../data/book_scrape_2', 'books_df2_p22')\n",
    "save_to_csv(books_df_2_p23, '../data/book_scrape_2', 'books_df2_p23')\n",
    "save_to_csv(books_df_2_p24, '../data/book_scrape_2', 'books_df2_p24')\n",
    "save_to_csv(books_df_2_p25, '../data/book_scrape_2', 'books_df2_p25')\n",
    "save_to_csv(books_df_2_p18, '../data/book_scrape_2', 'books_df2_p18')\n",
    "save_to_csv(books_df_2_p19, '../data/book_scrape_2', 'books_df2_p19')\n",
    "save_to_csv(books_df_2_p20, '../data/book_scrape_2', 'books_df2_p20')\n",
    "save_to_csv(books_df_2_p14, '../data/book_scrape_2', 'books_df2_p14')\n",
    "save_to_csv(books_df_2_p15, '../data/book_scrape_2', 'books_df2_p15')\n",
    "save_to_csv(books_df_2_p16, '../data/book_scrape_2', 'books_df2_p16')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80aab7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file saved to: /Users/quynhanh2004/Documents/GitHub/take-home-final-annepham1512/exams/final/data/books_data_2.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/quynhanh2004/Documents/GitHub/take-home-final-annepham1512/exams/final/data/books_data_2.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rewriting the function after reset\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def combine_csv_files(input_folder, output_file, prefix, num_files):\n",
    "    \"\"\"\n",
    "    Combines multiple CSV files in a specified order into a single CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        input_folder (str): Path to the folder containing the CSV files.\n",
    "        output_file (str): Path to the resulting combined CSV file.\n",
    "        prefix (str): Prefix of the CSV files (e.g., \"book_data_p\").\n",
    "        num_files (int): Total number of files to combine in sequential order.\n",
    "    \"\"\"\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for i in range(1, num_files + 1):\n",
    "        file_path = os.path.join(input_folder, f\"{prefix}{i}.csv\")\n",
    "        if os.path.exists(file_path):\n",
    "            temp_df = pd.read_csv(file_path)\n",
    "            combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"Combined CSV file saved to: {output_file}\")\n",
    "\n",
    "# Define the input folder and output file paths\n",
    "input_folder = \"/Users/quynhanh2004/Documents/GitHub/take-home-final-annepham1512/exams/final/data/book_scrape_2\"  # Adjusted path for execution environment\n",
    "output_file = \"/Users/quynhanh2004/Documents/GitHub/take-home-final-annepham1512/exams/final/data/books_data_2.csv\"  # Save the combined file outside the folder\n",
    "\n",
    "# Combine the CSV files\n",
    "combine_csv_files(input_folder, output_file, prefix=\"books_df2_p\", num_files=27)\n",
    "\n",
    "output_file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
